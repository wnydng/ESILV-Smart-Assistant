{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0vu6JPxVJhpN"
   },
   "source": [
    "# ‚úÇÔ∏è 2) Chunking intelligent (pr√©-processing pour le RAG)\n",
    "\n",
    "Une pipeline de d√©coupage avanc√©, qui :\n",
    "\n",
    "‚úî √âvite les coupures de mots\n",
    "\n",
    "‚úî √âvite les coupures de phrases\n",
    "\n",
    "‚úî G√©n√®re des segments coh√©rents (H2/H3, blocs logiques)\n",
    "\n",
    "‚úî Ajoute les m√©tadonn√©es :\n",
    "\n",
    "- rubric (admissions, formations‚Ä¶)\n",
    "- titre complet\n",
    "- URL parent + URL child\n",
    "- source file\n",
    "\n",
    "Le chunking cr√©e un dossier :\n",
    "chunks_esilv/ ‚Üí contenant tous les √©l√©ments pr√™ts pour ingestion RAG.\n",
    "\n",
    "üëâ Objectif atteint : `pr√©paration optimale du texte pour la vectorisation.`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-S-wB5by90N0"
   },
   "source": [
    "Ceraines phrases se rep√®tent donc :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bC-yKMaf9zkD"
   },
   "outputs": [],
   "source": [
    "def dedupe_sentences(text):\n",
    "    \"\"\"\n",
    "    Supprime les phrases r√©p√©t√©es dans un bloc de texte.\n",
    "    \"\"\"\n",
    "    if not text:\n",
    "        return \"\"\n",
    "\n",
    "    # Split par phrases\n",
    "    sentences = re.split(r\"(?<=[.!?])\\s+\", normalize(text))\n",
    "\n",
    "    seen = set()\n",
    "    clean_sentences = []\n",
    "\n",
    "    for s in sentences:\n",
    "        s_norm = normalize(s)\n",
    "        if not s_norm:\n",
    "            continue\n",
    "        if s_norm not in seen:\n",
    "            clean_sentences.append(s_norm)\n",
    "            seen.add(s_norm)\n",
    "\n",
    "    return \" \".join(clean_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 123,
     "status": "ok",
     "timestamp": 1765114048048,
     "user": {
      "displayName": "Lisa Naccache",
      "userId": "13815877186016026394"
     },
     "user_tz": -60
    },
    "id": "lF5xjWpuYPxG",
    "outputId": "f46de680-4c50-4eae-9731-5dfea3a57bc4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚û° 6 fichiers d√©tect√©s.\n",
      "üîç Traitement : formations.json\n",
      "‚úî G√©n√©r√© : /content/chunks_esilv/chunks_formations.json\n",
      "üîç Traitement : entreprises-debouches.json\n",
      "‚úî G√©n√©r√© : /content/chunks_esilv/chunks_entreprises-debouches.json\n",
      "üîç Traitement : recherche.json\n",
      "‚úî G√©n√©r√© : /content/chunks_esilv/chunks_recherche.json\n",
      "üîç Traitement : lecole.json\n",
      "‚úî G√©n√©r√© : /content/chunks_esilv/chunks_lecole.json\n",
      "üîç Traitement : international.json\n",
      "‚úî G√©n√©r√© : /content/chunks_esilv/chunks_international.json\n",
      "üîç Traitement : admissions.json\n",
      "‚úî G√©n√©r√© : /content/chunks_esilv/chunks_admissions.json\n",
      "\n",
      "üéâ Tous les chunks sont g√©n√©r√©s dans /content/chunks_esilv/\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import re\n",
    "\n",
    "INPUT_DIR = \"/content/scraping_esilv\"\n",
    "OUTPUT_DIR = \"/content/chunks_esilv\"\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "# URLs parentes officielles\n",
    "START_URLS = {\n",
    "    \"lecole\": \"https://www.esilv.fr/lecole/\",\n",
    "    \"admissions\": \"https://www.esilv.fr/admissions/\",\n",
    "    \"formations\": \"https://www.esilv.fr/formations/\",\n",
    "    \"entreprises-debouches\": \"https://www.esilv.fr/entreprises-debouches/\",\n",
    "    \"recherche\": \"https://www.esilv.fr/recherche/\",\n",
    "    \"international\": \"https://www.esilv.fr/international/\",\n",
    "}\n",
    "\n",
    "MAX_CHARS = 1000\n",
    "\n",
    "\n",
    "# ----------------------------------------------------\n",
    "# Normalisation s√©curis√©e\n",
    "# ----------------------------------------------------\n",
    "def normalize(t):\n",
    "    if not t:\n",
    "        return \"\"\n",
    "    return re.sub(r\"\\s+\", \" \", str(t)).strip()\n",
    "\n",
    "\n",
    "# ----------------------------------------------------\n",
    "# Transformer objet dict ‚Üí texte clair\n",
    "# ----------------------------------------------------\n",
    "def dict_to_text(obj):\n",
    "    parts = []\n",
    "\n",
    "    for key in [\"title\", \"content\", \"role\", \"phone\", \"email\"]:\n",
    "        if obj.get(key):\n",
    "            parts.append(normalize(obj[key]))\n",
    "\n",
    "    if obj.get(\"url\"):\n",
    "        parts.append(f\"(Plus d'informations : {obj['url']})\")\n",
    "\n",
    "    # join only non-empty\n",
    "    parts = [p for p in parts if p]\n",
    "    return \" ‚Äì \".join(parts)\n",
    "\n",
    "\n",
    "# ----------------------------------------------------\n",
    "# Chunking intelligent\n",
    "# ----------------------------------------------------\n",
    "def chunk_text(title, text, rubric=None, url=None):\n",
    "    text = normalize(text)\n",
    "    if not text:\n",
    "        return []\n",
    "\n",
    "    chunks = []\n",
    "\n",
    "    # chunk direct si petit\n",
    "    if len(text) <= MAX_CHARS:\n",
    "        chunks.append({\n",
    "            \"title\": title,\n",
    "            \"content\": text,\n",
    "            \"rubric\": rubric,\n",
    "            \"url\": url\n",
    "        })\n",
    "        return chunks\n",
    "\n",
    "    # d√©coupage par phrases\n",
    "    sentences = re.split(r\"(?<=[.!?])\\s+\", text)\n",
    "    buffer = \"\"\n",
    "\n",
    "    for sent in sentences:\n",
    "        sent = normalize(sent)\n",
    "        if not sent:\n",
    "            continue\n",
    "\n",
    "        if len(buffer) + len(sent) + 1 < MAX_CHARS:\n",
    "            buffer += sent + \" \"\n",
    "        else:\n",
    "            chunks.append({\n",
    "                \"title\": title,\n",
    "                \"content\": buffer.strip(),\n",
    "                \"rubric\": rubric,\n",
    "                \"url\": url\n",
    "            })\n",
    "            buffer = sent + \" \"\n",
    "\n",
    "    if buffer.strip():\n",
    "        chunks.append({\n",
    "            \"title\": title,\n",
    "            \"content\": buffer.strip(),\n",
    "            \"rubric\": rubric,\n",
    "            \"url\": url\n",
    "        })\n",
    "\n",
    "    return chunks\n",
    "\n",
    "\n",
    "# ----------------------------------------------------\n",
    "# Extraction chunks d'un fichier JSON scraping\n",
    "# ----------------------------------------------------\n",
    "def extract_chunks(data, parent_url):\n",
    "    rubric = data.get(\"rubric\", \"UNKNOWN\")\n",
    "    chunks = []\n",
    "\n",
    "    # -------------------------------------------\n",
    "    # INTRO\n",
    "    # -------------------------------------------\n",
    "    if data.get(\"intro\"):\n",
    "        chunks += chunk_text(\n",
    "            title=f\"{rubric} ‚Äì Introduction\",\n",
    "            text=data[\"intro\"],\n",
    "            rubric=rubric,\n",
    "            url=parent_url\n",
    "        )\n",
    "\n",
    "    # -------------------------------------------\n",
    "    # SECTIONS PRINCIPALES\n",
    "    # -------------------------------------------\n",
    "    for sec in data.get(\"sections\", []):\n",
    "        h2 = normalize(sec.get(\"h2\", \"\"))\n",
    "        h3 = normalize(sec.get(\"h3\"))\n",
    "        title = h2 if not h3 else f\"{h2} ‚Äì {h3}\"\n",
    "\n",
    "        text = \"\"\n",
    "        for item in sec.get(\"content\", []):\n",
    "            if isinstance(item, str):\n",
    "                text += normalize(item) + \" \"\n",
    "            elif isinstance(item, dict):\n",
    "                text += dict_to_text(item) + \" \"\n",
    "\n",
    "        #chunks += chunk_text(title, text, rubric, parent_url)\n",
    "        clean_text = dedupe_sentences(text)\n",
    "        chunks += chunk_text(title, clean_text, rubric, parent_url)\n",
    "\n",
    "    # -------------------------------------------\n",
    "    # CHILD PAGES\n",
    "    # -------------------------------------------\n",
    "    for cp in data.get(\"child_pages\", []):\n",
    "        cp_title = normalize(cp.get(\"title\", \"Page enfant\"))\n",
    "        cp_url = cp.get(\"url\")\n",
    "\n",
    "        # intro\n",
    "        if cp.get(\"intro\"):\n",
    "            chunks += chunk_text(\n",
    "                title=f\"{cp_title} ‚Äì Introduction\",\n",
    "                text=cp[\"intro\"],\n",
    "                rubric=rubric,\n",
    "                url=cp_url\n",
    "            )\n",
    "\n",
    "        # blocs\n",
    "        for block in cp.get(\"blocks\", []):\n",
    "            block_title = normalize(\n",
    "                block.get(\"h3\") or block.get(\"h4\") or block.get(\"title\") or cp_title\n",
    "            )\n",
    "\n",
    "            body = \"\"\n",
    "\n",
    "            # paragraphes\n",
    "            for p in block.get(\"text\", []):\n",
    "                if p:\n",
    "                    body += normalize(p) + \" \"\n",
    "\n",
    "            # titled_list\n",
    "            items = block.get(\"items\", [])\n",
    "            for it in items:\n",
    "                if it:\n",
    "                    body += normalize(it) + \" \"\n",
    "\n",
    "            # listes UL/LI\n",
    "            lists = block.get(\"lists\", [])\n",
    "            for lst in lists:\n",
    "                if isinstance(lst, list):\n",
    "                    for li in lst:\n",
    "                        if li:\n",
    "                            body += normalize(li) + \" \"\n",
    "\n",
    "            clean_body = dedupe_sentences(body)\n",
    "            chunks += chunk_text(\n",
    "                title=f\"{cp_title} ‚Äì {block_title}\",\n",
    "                text=clean_body,\n",
    "                rubric=rubric,\n",
    "                url=cp_url\n",
    "            )\n",
    "\n",
    "        # full text child\n",
    "\n",
    "        if cp.get(\"full_text\"):\n",
    "            chunks += chunk_text(\n",
    "                title=f\"{cp_title} ‚Äì Full text\",\n",
    "                text=cp[\"full_text\"],\n",
    "                rubric=rubric,\n",
    "                url=cp_url\n",
    "            )\n",
    "\n",
    "    return chunks\n",
    "\n",
    "\n",
    "# ----------------------------------------------------\n",
    "# PROCESS ALL\n",
    "# ----------------------------------------------------\n",
    "def process_all():\n",
    "    files = [f for f in os.listdir(INPUT_DIR) if f.endswith(\".json\")]\n",
    "    print(f\"‚û° {len(files)} fichiers d√©tect√©s.\")\n",
    "\n",
    "    for fn in files:\n",
    "        print(f\"üîç Traitement : {fn}\")\n",
    "\n",
    "        key = fn.replace(\".json\", \"\")\n",
    "        parent_url = START_URLS.get(key)\n",
    "\n",
    "        with open(os.path.join(INPUT_DIR, fn), \"r\", encoding=\"utf-8\") as f:\n",
    "            data = json.load(f)\n",
    "\n",
    "        chunks = extract_chunks(data, parent_url)\n",
    "\n",
    "        out_path = os.path.join(OUTPUT_DIR, f\"chunks_{fn}\")\n",
    "        with open(out_path, \"w\", encoding=\"utf-8\") as f:\n",
    "            json.dump(chunks, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "        print(f\"‚úî G√©n√©r√© : {out_path}\")\n",
    "\n",
    "    print(\"\\nüéâ Tous les chunks sont g√©n√©r√©s dans /content/chunks_esilv/\")\n",
    "\n",
    "\n",
    "# RUN\n",
    "process_all()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MgYuly3g_Yhv"
   },
   "source": [
    "plus aucune phrase r√©p√©t√©e\n",
    "\n",
    "chunks ultra propres\n",
    "\n",
    "embeddings beaucoup plus efficaces\n",
    "\n",
    "meilleur RAG\n",
    "\n",
    "meilleure coh√©rence des r√©ponses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 82,
     "status": "ok",
     "timestamp": 1765114297040,
     "user": {
      "displayName": "Lisa Naccache",
      "userId": "13815877186016026394"
     },
     "user_tz": -60
    },
    "id": "ceiYeFoRAqwq",
    "outputId": "244b53a0-e87a-4ecb-a232-762ab803c10e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üßπ 6 fichiers d√©tect√©s √† nettoyer\n",
      "üîç Nettoyage : chunks_lecole.json\n",
      "‚úî Fichier nettoy√© ‚Üí /content/chunks_esilv_clean/chunks_lecole.json\n",
      "üîç Nettoyage : chunks_recherche.json\n",
      "‚úî Fichier nettoy√© ‚Üí /content/chunks_esilv_clean/chunks_recherche.json\n",
      "üîç Nettoyage : chunks_formations.json\n",
      "‚úî Fichier nettoy√© ‚Üí /content/chunks_esilv_clean/chunks_formations.json\n",
      "üîç Nettoyage : chunks_international.json\n",
      "‚úî Fichier nettoy√© ‚Üí /content/chunks_esilv_clean/chunks_international.json\n",
      "üîç Nettoyage : chunks_entreprises-debouches.json\n",
      "‚úî Fichier nettoy√© ‚Üí /content/chunks_esilv_clean/chunks_entreprises-debouches.json\n",
      "üîç Nettoyage : chunks_admissions.json\n",
      "‚úî Fichier nettoy√© ‚Üí /content/chunks_esilv_clean/chunks_admissions.json\n",
      "\n",
      "üéâ Nettoyage COMPLET termin√© !\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import re\n",
    "\n",
    "CHUNKS_DIR = \"/content/chunks_esilv\"\n",
    "CLEAN_DIR = \"/content/chunks_esilv_clean\"\n",
    "os.makedirs(CLEAN_DIR, exist_ok=True)\n",
    "\n",
    "\n",
    "# -------------------------------\n",
    "# Nettoyage d‚Äôun chunk individuel\n",
    "# -------------------------------\n",
    "def clean_chunk_content(text):\n",
    "    if not text:\n",
    "        return \"\"\n",
    "\n",
    "    # Normalisation espaces\n",
    "    text = re.sub(r\"\\s+\", \" \", text).strip()\n",
    "\n",
    "    # D√©couper en phrases\n",
    "    sentences = re.split(r\"(?<=[.!?])\\s+\", text)\n",
    "\n",
    "    # Supprimer doublons internes\n",
    "    seen = set()\n",
    "    cleaned_sentences = []\n",
    "    for s in sentences:\n",
    "        s_norm = s.lower().strip()\n",
    "        if s_norm and s_norm not in seen:\n",
    "            cleaned_sentences.append(s)\n",
    "            seen.add(s_norm)\n",
    "\n",
    "    cleaned = \". \".join(cleaned_sentences).strip()\n",
    "\n",
    "    # Petite correction : √©viter \". .\" dans les jonctions\n",
    "    cleaned = cleaned.replace(\". .\", \".\")\n",
    "\n",
    "    return cleaned\n",
    "\n",
    "\n",
    "# -------------------------------\n",
    "# Nettoyer un fichier de chunks\n",
    "# -------------------------------\n",
    "def clean_chunks_file(path):\n",
    "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "        chunks = json.load(f)\n",
    "\n",
    "    cleaned = []\n",
    "    global_seen = set()\n",
    "\n",
    "    for chunk in chunks:\n",
    "        text = clean_chunk_content(chunk.get(\"content\", \"\"))\n",
    "\n",
    "        # supprimer chunks vides ou ultra-courts\n",
    "        if len(text) < 20:\n",
    "            continue\n",
    "\n",
    "        # doublons globaux\n",
    "        hash_text = text.lower().strip()\n",
    "        if hash_text in global_seen:\n",
    "            continue\n",
    "\n",
    "        global_seen.add(hash_text)\n",
    "\n",
    "        # chunk nettoy√©\n",
    "        cleaned.append({\n",
    "            \"title\": chunk.get(\"title\", \"\"),\n",
    "            \"content\": text,\n",
    "            \"rubric\": chunk.get(\"rubric\", \"\"),\n",
    "            \"url\": chunk.get(\"url\", None)\n",
    "        })\n",
    "\n",
    "    return cleaned\n",
    "\n",
    "\n",
    "# -------------------------------\n",
    "# PROCESS ALL\n",
    "# -------------------------------\n",
    "def process_all():\n",
    "    files = [f for f in os.listdir(CHUNKS_DIR) if f.endswith(\".json\")]\n",
    "    print(f\"üßπ {len(files)} fichiers d√©tect√©s √† nettoyer\")\n",
    "\n",
    "    for fn in files:\n",
    "        input_path = os.path.join(CHUNKS_DIR, fn)\n",
    "        output_path = os.path.join(CLEAN_DIR, fn)\n",
    "\n",
    "        print(f\"üîç Nettoyage : {fn}\")\n",
    "\n",
    "        cleaned_chunks = clean_chunks_file(input_path)\n",
    "\n",
    "        with open(output_path, \"w\", encoding=\"utf-8\") as f:\n",
    "            json.dump(cleaned_chunks, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "        print(f\"‚úî Fichier nettoy√© ‚Üí {output_path}\")\n",
    "\n",
    "    print(\"\\nüéâ Nettoyage COMPLET termin√© !\")\n",
    "\n",
    "\n",
    "process_all()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hrMo3S4q5LE_"
   },
   "source": [
    "- Nettoie proprement\n",
    "- Transforme les objets en bon texte\n",
    "- D√©coupe sans couper les mots\n",
    "- Respecte H2 / H3 / blocs / child pages\n",
    "- Est compatible avec FAISS, ChromaDB, Ollama embeddings\n",
    "- G√®re 1 fichier par fichier ‚Üí rapide pour ta machine"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyNQx81G0jq/CBrCXk3Wcwfv",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
