{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"collapsed_sections":["1nh7oM9-YyqH"],"mount_file_id":"1Ftnr3QQZC2jjYkQLvIDp6W9Qxv6NMg27","authorship_tag":"ABX9TyMOE52KU2qbEL4GGXIChMDE"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# ðŸ§± 1) Scraping ciblÃ© du site ESILV\n","\n","Scraper robuste qui extrait automatiquement :\n","\n","- les rubriques principales (Ã©cole, admissions, formations, etc.)\n","\n","- les sous-rubriques\n","\n","- les pages enfants (child pages)\n","\n","- les informations structurÃ©es :\n","\n","âœ” titres\n","\n","âœ” contenus textuels\n","\n","âœ” listes\n","\n","âœ” contacts\n","\n","âœ” URL\n","\n","âœ” full text pour la recherche sÃ©mantique\n","\n","Le scraper produit un JSON propre et structurÃ©, utilisable pour un modÃ¨le de langage.\n","\n","ðŸ‘‰ Objectif atteint : `transformation dâ€™un site web en base de connaissances exploitable.`"],"metadata":{"id":"hRcl6td5KH7T"}},{"cell_type":"code","source":["import requests\n","import trafilatura\n","import xml.etree.ElementTree as ET\n","import json\n","import time\n","import requests\n","from bs4 import BeautifulSoup\n","from urllib.parse import urljoin\n","import json\n","import re\n","import os\n","import traceback"],"metadata":{"id":"AZfs8_U3l_Jp"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["pip install trafilatura"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"collapsed":true,"id":"_uIz6GwgmDxz","executionInfo":{"status":"ok","timestamp":1764771789294,"user_tz":-60,"elapsed":7772,"user":{"displayName":"Lisa Naccache","userId":"13815877186016026394"}},"outputId":"cca8fdfd-92af-44ce-9956-d8c4676dfe74"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting trafilatura\n","  Downloading trafilatura-2.0.0-py3-none-any.whl.metadata (12 kB)\n","Requirement already satisfied: certifi in /usr/local/lib/python3.12/dist-packages (from trafilatura) (2025.11.12)\n","Requirement already satisfied: charset_normalizer>=3.4.0 in /usr/local/lib/python3.12/dist-packages (from trafilatura) (3.4.4)\n","Collecting courlan>=1.3.2 (from trafilatura)\n","  Downloading courlan-1.3.2-py3-none-any.whl.metadata (17 kB)\n","Collecting htmldate>=1.9.2 (from trafilatura)\n","  Downloading htmldate-1.9.4-py3-none-any.whl.metadata (10 kB)\n","Collecting justext>=3.0.1 (from trafilatura)\n","  Downloading justext-3.0.2-py2.py3-none-any.whl.metadata (7.3 kB)\n","Requirement already satisfied: lxml>=5.3.0 in /usr/local/lib/python3.12/dist-packages (from trafilatura) (6.0.2)\n","Requirement already satisfied: urllib3<3,>=1.26 in /usr/local/lib/python3.12/dist-packages (from trafilatura) (2.5.0)\n","Requirement already satisfied: babel>=2.16.0 in /usr/local/lib/python3.12/dist-packages (from courlan>=1.3.2->trafilatura) (2.17.0)\n","Collecting tld>=0.13 (from courlan>=1.3.2->trafilatura)\n","  Downloading tld-0.13.1-py2.py3-none-any.whl.metadata (10 kB)\n","Collecting dateparser>=1.1.2 (from htmldate>=1.9.2->trafilatura)\n","  Downloading dateparser-1.2.2-py3-none-any.whl.metadata (29 kB)\n","Requirement already satisfied: python-dateutil>=2.9.0.post0 in /usr/local/lib/python3.12/dist-packages (from htmldate>=1.9.2->trafilatura) (2.9.0.post0)\n","Requirement already satisfied: pytz>=2024.2 in /usr/local/lib/python3.12/dist-packages (from dateparser>=1.1.2->htmldate>=1.9.2->trafilatura) (2025.2)\n","Requirement already satisfied: regex>=2024.9.11 in /usr/local/lib/python3.12/dist-packages (from dateparser>=1.1.2->htmldate>=1.9.2->trafilatura) (2025.11.3)\n","Requirement already satisfied: tzlocal>=0.2 in /usr/local/lib/python3.12/dist-packages (from dateparser>=1.1.2->htmldate>=1.9.2->trafilatura) (5.3.1)\n","Collecting lxml_html_clean (from lxml[html_clean]>=4.4.2->justext>=3.0.1->trafilatura)\n","  Downloading lxml_html_clean-0.4.3-py3-none-any.whl.metadata (2.3 kB)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.9.0.post0->htmldate>=1.9.2->trafilatura) (1.17.0)\n","Downloading trafilatura-2.0.0-py3-none-any.whl (132 kB)\n","\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m132.6/132.6 kB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading courlan-1.3.2-py3-none-any.whl (33 kB)\n","Downloading htmldate-1.9.4-py3-none-any.whl (31 kB)\n","Downloading justext-3.0.2-py2.py3-none-any.whl (837 kB)\n","\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m837.9/837.9 kB\u001b[0m \u001b[31m20.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading dateparser-1.2.2-py3-none-any.whl (315 kB)\n","\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m315.5/315.5 kB\u001b[0m \u001b[31m18.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading tld-0.13.1-py2.py3-none-any.whl (274 kB)\n","\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m274.7/274.7 kB\u001b[0m \u001b[31m15.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading lxml_html_clean-0.4.3-py3-none-any.whl (14 kB)\n","Installing collected packages: tld, lxml_html_clean, dateparser, courlan, justext, htmldate, trafilatura\n","Successfully installed courlan-1.3.2 dateparser-1.2.2 htmldate-1.9.4 justext-3.0.2 lxml_html_clean-0.4.3 tld-0.13.1 trafilatura-2.0.0\n"]}]},{"cell_type":"markdown","source":["## ANCIENNE VERSION"],"metadata":{"id":"1nh7oM9-YyqH"}},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"5rpdzJobl9KO","executionInfo":{"status":"ok","timestamp":1764773926161,"user_tz":-60,"elapsed":18322,"user":{"displayName":"Lisa Naccache","userId":"13815877186016026394"}},"outputId":"55985386-cef1-4e92-dd26-259c096beaaa"},"outputs":[{"output_type":"stream","name":"stdout","text":["\n","=== SCRAPING RUBRIQUE : LECOLE ===\n","âœ” Rubrique lecole : fichier JSON crÃ©Ã©.\n","ðŸŽ‰ Rubrique lecole terminÃ©e.\n","\n","\n","=== SCRAPING RUBRIQUE : ADMISSIONS ===\n","âœ” Rubrique admissions : fichier JSON crÃ©Ã©.\n","ðŸŽ‰ Rubrique admissions terminÃ©e.\n","\n","\n","=== SCRAPING RUBRIQUE : FORMATIONS ===\n","âœ” Rubrique formations : fichier JSON crÃ©Ã©.\n","ðŸŽ‰ Rubrique formations terminÃ©e.\n","\n","\n","=== SCRAPING RUBRIQUE : ENTREPRISES-DEBOUCHES ===\n","âœ” Rubrique entreprises-debouches : fichier JSON crÃ©Ã©.\n","ðŸŽ‰ Rubrique entreprises-debouches terminÃ©e.\n","\n","\n","=== SCRAPING RUBRIQUE : RECHERCHE ===\n","âœ” Rubrique recherche : fichier JSON crÃ©Ã©.\n","ðŸŽ‰ Rubrique recherche terminÃ©e.\n","\n","\n","=== SCRAPING RUBRIQUE : INTERNATIONAL ===\n","âœ” Rubrique international : fichier JSON crÃ©Ã©.\n","ðŸŽ‰ Rubrique international terminÃ©e.\n","\n","\n","ðŸ SCRAPING COMPLET PAR RUBRIQUE TERMINÃ‰ !\n"]}],"source":["import requests\n","from bs4 import BeautifulSoup\n","import trafilatura\n","import json\n","import os\n","from urllib.parse import urljoin\n","\n","START_URLS = {\n","    \"lecole\": \"https://www.esilv.fr/lecole/\",\n","    \"admissions\": \"https://www.esilv.fr/admissions/\",\n","    \"formations\": \"https://www.esilv.fr/formations/\",\n","    \"entreprises-debouches\": \"https://www.esilv.fr/entreprises-debouches/\",\n","    \"recherche\": \"https://www.esilv.fr/recherche/\",\n","    \"international\": \"https://www.esilv.fr/international/\",\n","}\n","\n","\n","def get_soup(url):\n","    r = requests.get(url, timeout=10)\n","    return BeautifulSoup(r.text, \"html.parser\")\n","\n","\n","def extract_main_rubric_data(soup, base_url):\n","    \"\"\"Extrait h4, h2, h3 + div content dans la rubrique principale\"\"\"\n","\n","    data = {\n","        \"context_h4\": [h.get_text(strip=True) for h in soup.select(\".cat_wrapper h4\")],\n","        \"titles_h2\": [h.get_text(strip=True) for h in soup.select(\".cat_wrapper h2\")],\n","        \"subtitles_h3\": [h.get_text(strip=True) for h in soup.select(\".cat_wrapper h3\")],\n","        \"paragraphs\": [p.get_text(strip=True) for p in soup.select(\".cat_wrapper p\")],\n","    }\n","\n","    return data\n","\n","\n","def extract_subrubrics(soup, base_url):\n","    \"\"\"Extrait les sous-rubriques avec post_header half + post_excerpt half\"\"\"\n","\n","    subrubrics = []\n","\n","    cards = soup.select(\".half\")  # Les blocs ESILV sont souvent en half columns\n","\n","    for card in cards:\n","        # Nom de la sous-rubrique (dans post_header)\n","        title_tag = card.select_one(\".post-header h2, .post-header h3, .post-header h1\")\n","        title = title_tag.get_text(strip=True) if title_tag else None\n","\n","        # RÃ©sumÃ© court\n","        excerpt_tag = card.select_one(\".post-excerpt p\")\n","        excerpt = excerpt_tag.get_text(strip=True) if excerpt_tag else None\n","\n","        # Lien vers la sous-page\n","        link_tag = card.select_one(\".post-excerpt a\")\n","        if link_tag and link_tag.get(\"href\"):\n","            url = urljoin(base_url, link_tag[\"href\"])\n","        else:\n","            url = None\n","\n","        if title or excerpt or url:\n","            subrubrics.append({\n","                \"title\": title,\n","                \"excerpt\": excerpt,\n","                \"url\": url\n","            })\n","\n","    return subrubrics\n","\n","\n","def extract_contacts(soup):\n","    \"\"\"Extraction de la section contacts\"\"\"\n","\n","    contacts_block = soup.select_one(\".page_contacts\")\n","    if not contacts_block:\n","        return None\n","\n","    contacts = []\n","    for person in contacts_block.select(\".contact\"):\n","        name = person.select_one(\"h3\")\n","        role = person.select_one(\"p\")\n","        phone = person.select_one(\"a[href^='tel']\")\n","        email = person.select_one(\"a[href^='mailto']\")\n","\n","        contacts.append({\n","            \"name\": name.get_text(strip=True) if name else None,\n","            \"role\": role.get_text(strip=True) if role else None,\n","            \"phone\": phone.get_text(strip=True) if phone else None,\n","            \"email\": email.get_text(strip=True) if email else None,\n","        })\n","\n","    return contacts\n","\n","\n","def scrape_page_content(url):\n","    \"\"\"Scrape le contenu en texte propre via trafilatura.\"\"\"\n","    html = trafilatura.fetch_url(url)\n","    if html:\n","        return trafilatura.extract(html)\n","    return None\n","\n","\n","def scrape_rubric(name, url):\n","\n","    print(f\"\\n=== SCRAPING RUBRIQUE : {name.upper()} ===\")\n","\n","    folder = f\"scraping/{name}\"\n","    os.makedirs(folder, exist_ok=True)\n","\n","    soup = get_soup(url)\n","\n","    # 1ï¸âƒ£ Extraire les informations principales de la rubrique\n","    main_info = extract_main_rubric_data(soup, url)\n","\n","    # 2ï¸âƒ£ Extraire les sous-rubriques\n","    subrubrics = extract_subrubrics(soup, url)\n","\n","    # 3ï¸âƒ£ Extraire section contacts\n","    contacts = extract_contacts(soup)\n","\n","    # 4ï¸âƒ£ Scraper le contenu principal en texte propre\n","    main_text = scrape_page_content(url)\n","\n","    rubric_json = {\n","        \"rubric\": name,\n","        \"url\": url,\n","        \"summary\": main_info,\n","        \"content_text\": main_text,\n","        \"subrubrics\": subrubrics,\n","        \"contacts\": contacts\n","    }\n","\n","    # SAVE main file\n","    with open(f\"{folder}/{name}.json\", \"w\", encoding=\"utf-8\") as f:\n","        json.dump(rubric_json, f, indent=2, ensure_ascii=False)\n","\n","    print(f\"âœ” Rubrique {name} : fichier JSON crÃ©Ã©.\")\n","\n","    # 5ï¸âƒ£ Scraper chaque sous-rubrique dans un fichier sÃ©parÃ©\n","    for sub in subrubrics:\n","        if not sub[\"url\"]:\n","            continue\n","\n","        print(\"âž¡ï¸ Sous-rubrique :\", sub[\"title\"])\n","\n","        sub_text = scrape_page_content(sub[\"url\"])\n","\n","        sub_json = {\n","            \"title\": sub[\"title\"],\n","            \"url\": sub[\"url\"],\n","            \"excerpt\": sub[\"excerpt\"],\n","            \"content_text\": sub_text\n","        }\n","\n","        filename = sub[\"title\"].lower().replace(\" \", \"-\").replace(\"Ã©\", \"e\")\n","        with open(f\"{folder}/{filename}.json\", \"w\", encoding=\"utf-8\") as f:\n","            json.dump(sub_json, f, indent=2, ensure_ascii=False)\n","\n","    print(f\"ðŸŽ‰ Rubrique {name} terminÃ©e.\\n\")\n","\n","\n","# === EXECUTION ===\n","for name, url in START_URLS.items():\n","    scrape_rubric(name, url)\n","\n","print(\"\\nðŸ SCRAPING COMPLET PAR RUBRIQUE TERMINÃ‰ !\")\n"]},{"cell_type":"code","source":["URL = \"https://www.esilv.fr/admissions/\"\n","\n","def scrape_admissions(url):\n","    r = requests.get(url)\n","    soup = BeautifulSoup(r.text, \"html.parser\")\n","\n","    data = {}\n","\n","    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n","    # INTRO\n","    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n","    intro_h4 = soup.select_one(\".standard_wrapper.cat_wrapper > h4\")\n","    data[\"intro\"] = intro_h4.get_text(strip=True) if intro_h4 else None\n","\n","    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n","    # LISTE DES SUBRUBRICS (cartes)\n","    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n","    subrubrics = []\n","\n","    for card in soup.select(\".post_wrapper.one_half\"):\n","\n","        title_tag = card.select_one(\".post_header.half h3 a\")\n","        excerpt_tag = card.select_one(\".post_excerpt.half p\")\n","        btn = card.select_one(\".post_excerpt.half a[href]\")\n","\n","        subrubrics.append({\n","            \"title\": title_tag.get_text(strip=True) if title_tag else None,\n","            \"content\": excerpt_tag.get_text(strip=True) if excerpt_tag else None,\n","            \"url\": urljoin(url, btn[\"href\"]) if btn else None\n","        })\n","\n","    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n","    # CONTACTS\n","    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n","    contacts = []\n","\n","    for c in soup.select(\"#page_contacts .one_fifth\"):\n","        name = c.find(\"h3\")\n","        role = c.find(\"h4\")\n","        phone_raw = c.find(\"p\")\n","        email_tag = c.find(\"a\", href=lambda h: h and \"mailto\" in h)\n","\n","        phone = None\n","        if phone_raw:\n","            phone = phone_raw.get_text(strip=True).split(\"\\n\")[0]\n","\n","        contacts.append({\n","            \"name\": name.get_text(separator=\" \", strip=True) if name else None,\n","            \"role\": role.get_text(separator=\" \", strip=True) if role else None,\n","            \"phone\": phone,\n","            \"email\": email_tag[\"href\"].replace(\"mailto:\", \"\") if email_tag else None\n","        })\n","\n","    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n","    # SECTIONS H2 (avec contenu rÃ©assignÃ©)\n","    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n","    sections = []\n","\n","    for h2 in soup.select(\".standard_wrapper.cat_wrapper h2\"):\n","\n","        title = h2.get_text(strip=True)\n","\n","        section = {\"h2\": title, \"h3\": None, \"content\": []}\n","\n","        # H3 juste aprÃ¨s ?\n","        nxt = h2.find_next_sibling()\n","        if nxt and nxt.name == \"h3\":\n","            section[\"h3\"] = nxt.get_text(strip=True)\n","\n","        # Cas 1 : si le h2 = les procÃ©dures â†’ content = subrubrics\n","        if \"procÃ©dures\" in title.lower():\n","            section[\"content\"] = subrubrics\n","\n","        # Cas 2 : si le h2 = Vos contacts â†’ content = contacts\n","        elif \"contacts\" in title.lower():\n","            section[\"content\"] = contacts\n","\n","        # Cas 3 : sinon â†’ extraire les DIV/MOTS AVANT le prochain h2/hr\n","        else:\n","            cur = h2.find_next_sibling()\n","            while cur and cur.name not in [\"h2\", \"hr\"]:\n","                if cur.name == \"div\" and cur.get(\"style\"):\n","                    section[\"content\"].append(cur.get_text(strip=True))\n","                cur = cur.find_next_sibling()\n","\n","        sections.append(section)\n","\n","    data[\"sections\"] = sections\n","\n","    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n","    # FULL TEXT\n","    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n","    html = trafilatura.fetch_url(url)\n","    data[\"full_text\"] = trafilatura.extract(html) if html else None\n","\n","    return data\n","\n","\n","# EXECUTION\n","result = scrape_admissions(URL)\n","# CLEANING\n","#result = clean_json_data(result)\n","\n","# CHUNKING\n","#result[\"chunks\"] = chunk_rag(result)\n","\n","with open(\"admissions_final.json\", \"w\", encoding=\"utf-8\") as f:\n","    json.dump(result, f, indent=2, ensure_ascii=False)\n","\n","print(\"âœ” admissions_final.json crÃ©Ã© avec succÃ¨s !\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":356},"id":"9r2GMgkowI1B","executionInfo":{"status":"error","timestamp":1764781834003,"user_tz":-60,"elapsed":1439,"user":{"displayName":"Lisa Naccache","userId":"13815877186016026394"}},"outputId":"00d26c13-2970-4cd5-b89c-90eb072f68dd"},"execution_count":null,"outputs":[{"output_type":"error","ename":"TypeError","evalue":"unhashable type: 'dict'","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m/tmp/ipython-input-3815916935.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    100\u001b[0m \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mscrape_admissions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mURL\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m \u001b[0;31m# CLEANING\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 102\u001b[0;31m \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclean_json_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    103\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m \u001b[0;31m# CHUNKING\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/tmp/ipython-input-247389190.py\u001b[0m in \u001b[0;36mclean_json_data\u001b[0;34m(data)\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msec\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"content\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 36\u001b[0;31m             \u001b[0msec\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"content\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclean_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msec\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"content\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     37\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m     \u001b[0;31m# Nettoyage child_pages\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/tmp/ipython-input-247389190.py\u001b[0m in \u001b[0;36mclean_list\u001b[0;34m(lst)\u001b[0m\n\u001b[1;32m     19\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mitem\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlst\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m             \u001b[0mc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclean_value\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m             \u001b[0;32mif\u001b[0m \u001b[0mc\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mseen\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m                 \u001b[0mseen\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m                 \u001b[0mcleaned\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mTypeError\u001b[0m: unhashable type: 'dict'"]}]},{"cell_type":"code","source":["import requests\n","from bs4 import BeautifulSoup\n","import json\n","from urllib.parse import urljoin\n","import trafilatura\n","\n","URL = \"https://www.esilv.fr/admissions/\"\n","\n","\n","# ----------------------------------------------------------\n","# UTILITAIRE : Nettoyage simple\n","# ----------------------------------------------------------\n","def clean(text):\n","    if not text:\n","        return None\n","    return \" \".join(text.split()).strip()\n","\n","\n","# ----------------------------------------------------------\n","# SCRAPER ADMISSIONS\n","# ----------------------------------------------------------\n","def scrape_admissions(url):\n","    r = requests.get(url)\n","    soup = BeautifulSoup(r.text, \"html.parser\")\n","\n","    data = {}\n","\n","    # Rubrique en titre principal du JSON\n","    data[\"rubric\"] = \"Admissions\"\n","\n","    # ------------------------------------------------------\n","    # INTRODUCTION <h4>\n","    # ------------------------------------------------------\n","    intro_h4 = soup.select_one(\".standard_wrapper.cat_wrapper > h4\")\n","    data[\"intro\"] = clean(intro_h4.get_text()) if intro_h4 else None\n","\n","    # ------------------------------------------------------\n","    # SUBRUBRICS (cartes)\n","    # ------------------------------------------------------\n","    subrubrics = []\n","\n","    for card in soup.select(\".post_wrapper.one_half\"):\n","\n","        title_tag = card.select_one(\".post_header.half h3 a\")\n","        excerpt_tag = card.select_one(\".post_excerpt.half p\")\n","        btn = card.select_one(\".post_excerpt.half a[href]\")\n","\n","        subrubrics.append({\n","            \"title\": clean(title_tag.get_text()) if title_tag else None,\n","            \"content\": clean(excerpt_tag.get_text()) if excerpt_tag else None,\n","            \"url\": urljoin(url, btn[\"href\"]) if btn else None\n","        })\n","\n","    # ------------------------------------------------------\n","    # CONTACTS\n","    # ------------------------------------------------------\n","    contacts = []\n","\n","    for c in soup.select(\"#page_contacts .one_fifth\"):\n","        name = c.find(\"h3\")\n","        role = c.find(\"h4\")\n","        phone_raw = c.find(\"p\")\n","        email_tag = c.find(\"a\", href=lambda h: h and \"mailto\" in h)\n","\n","        phone = None\n","        if phone_raw:\n","            phone = clean(phone_raw.get_text()).split(\" \")[0]\n","\n","        contacts.append({\n","            \"name\": clean(name.get_text(separator=\" \")) if name else None,\n","            \"role\": clean(role.get_text(separator=\" \")) if role else None,\n","            \"phone\": phone,\n","            \"email\": email_tag[\"href\"].replace(\"mailto:\", \"\") if email_tag else None\n","        })\n","\n","    # ------------------------------------------------------\n","    # SECTIONS H2 (avec contenu organisÃ© et nettoyÃ©)\n","    # ------------------------------------------------------\n","    sections = []\n","\n","    for h2 in soup.select(\".standard_wrapper.cat_wrapper h2\"):\n","\n","        title = clean(h2.get_text())\n","        section = {\"h2\": title}\n","\n","        # Cherche un Ã©ventuel H3 juste aprÃ¨s\n","        nxt = h2.find_next_sibling()\n","        h3 = clean(nxt.get_text()) if nxt and nxt.name == \"h3\" else None\n","        if h3:\n","            section[\"h3\"] = h3  # on ajoute seulement si non nul\n","\n","        # Cas : \"Les procÃ©dures en dÃ©tails\" â†’ mettre subrubrics\n","        if \"procÃ©dure\" in title.lower():\n","            section[\"content\"] = subrubrics\n","\n","        # Cas : \"Vos contacts admissions\"\n","        elif \"contacts\" in title.lower():\n","            section[\"content\"] = contacts\n","\n","        # Autres sections â†’ on extrait les div[style]\n","        else:\n","            content_list = []\n","            cur = h2.find_next_sibling()\n","\n","            while cur and cur.name not in [\"h2\", \"hr\"]:\n","                if cur.name == \"div\" and cur.get(\"style\"):\n","                    cleaned = clean(cur.get_text())\n","                    if cleaned:\n","                        content_list.append(cleaned)\n","                cur = cur.find_next_sibling()\n","\n","            section[\"content\"] = content_list\n","\n","        # Nettoyer les champs vides\n","        section = {k: v for k, v in section.items() if v}\n","\n","        sections.append(section)\n","\n","    data[\"sections\"] = sections\n","\n","    # ------------------------------------------------------\n","    # FULL TEXT POUR LE RAG\n","    # ------------------------------------------------------\n","    html = trafilatura.fetch_url(url)\n","    data[\"full_text\"] = clean(trafilatura.extract(html)) if html else None\n","\n","    return data\n","\n","\n","# ----------------------------------------------------------\n","# EXECUTION\n","# ----------------------------------------------------------\n","result = scrape_admissions(URL)\n","\n","with open(\"admissions_cleaned.json\", \"w\", encoding=\"utf-8\") as f:\n","    json.dump(result, f, indent=2, ensure_ascii=False)\n","\n","print(\"âœ” admissions_cleaned.json gÃ©nÃ©rÃ© avec succÃ¨s !\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"c4JK0YP22E80","executionInfo":{"status":"ok","timestamp":1764775979886,"user_tz":-60,"elapsed":1461,"user":{"displayName":"Lisa Naccache","userId":"13815877186016026394"}},"outputId":"e7d6db46-3518-4728-eb40-0f95c9446d8c"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["âœ” admissions_cleaned.json gÃ©nÃ©rÃ© avec succÃ¨s !\n"]}]},{"cell_type":"code","source":["import requests\n","from bs4 import BeautifulSoup\n","from urllib.parse import urljoin\n","import trafilatura\n","import json\n","import re\n","\n","\n","def clean_text(t):\n","    if not t:\n","        return None\n","    t = re.sub(r\"\\s+\", \" \", t)\n","    return t.strip()\n","\n","\n","# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n","# SCRAPER STANDARD POUR TOUTE PAGE ENFANT\n","# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n","def scrape_child_page(url):\n","    r = requests.get(url)\n","    soup = BeautifulSoup(r.text, \"html.parser\")\n","\n","    result = {\n","        \"url\": url,\n","        \"title\": None,\n","        \"intro\": None,\n","        \"blocks\": []\n","    }\n","\n","    # h2 principal\n","    h2 = soup.select_one(\".standard_wrapper.cat_wrapper h2\")\n","    if h2:\n","        result[\"title\"] = clean_text(h2.get_text())\n","\n","    # h4 introductif\n","    intro = soup.select_one(\".standard_wrapper.cat_wrapper > h4\")\n","    if intro:\n","        result[\"intro\"] = clean_text(intro.get_text())\n","\n","    # SÃ©lectionner toutes les zones .one_half (structure commune ESILV)\n","    for zone in soup.select(\".one_half, .one_half.last\"):\n","\n","        block = {\n","            \"h3\": None,\n","            \"h4\": None,\n","            \"text\": [],\n","            \"list\": []\n","        }\n","\n","        h3 = zone.find(\"h3\")\n","        h4 = zone.find(\"h4\")\n","\n","        if h3:\n","            block[\"h3\"] = clean_text(h3.get_text())\n","\n","        if h4:\n","            block[\"h4\"] = clean_text(h4.get_text())\n","\n","        # Paragraphes\n","        for p in zone.find_all(\"p\"):\n","            txt = clean_text(p.get_text())\n","            if txt:\n","                block[\"text\"].append(txt)\n","\n","        # Listes UL\n","        for ul in zone.find_all(\"ul\"):\n","            items = [clean_text(li.get_text()) for li in ul.find_all(\"li\")]\n","            block[\"list\"].append([i for i in items if i])\n","\n","        # Ne pas ajouter les blocs vides\n","        if block[\"h3\"] or block[\"h4\"] or block[\"text\"] or block[\"list\"]:\n","            result[\"blocks\"].append(block)\n","\n","    # Extraction du texte global\n","    html = trafilatura.fetch_url(url)\n","    result[\"full_text\"] = trafilatura.extract(html) if html else None\n","\n","    return result\n","\n","\n","# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n","# SCRAPER PRINCIPAL ADMISSIONS\n","# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n","def scrape_admissions(url):\n","    r = requests.get(url)\n","    soup = BeautifulSoup(r.text, \"html.parser\")\n","\n","    data = {\n","        \"rubric\": \"Admissions\",\n","        \"intro\": None,\n","        \"sections\": []\n","    }\n","\n","    # INTRO\n","    intro_h4 = soup.select_one(\".standard_wrapper.cat_wrapper > h4\")\n","    if intro_h4:\n","        data[\"intro\"] = clean_text(intro_h4.get_text())\n","\n","    # SUBRUBRICS\n","    subrubrics = []\n","    for card in soup.select(\".post_wrapper.one_half\"):\n","\n","        title = card.select_one(\".post_header.half h3 a\")\n","        content = card.select_one(\".post_excerpt.half p\")\n","        btn = card.select_one(\".post_excerpt.half a[href]\")\n","\n","        item = {\n","            \"title\": clean_text(title.get_text()) if title else None,\n","            \"content\": clean_text(content.get_text()) if content else None,\n","            \"url\": urljoin(url, btn[\"href\"]) if btn else None,\n","            \"child_page\": None   # <-- important : on la remplira aprÃ¨s\n","        }\n","\n","        subrubrics.append(item)\n","\n","    # CONTACTS\n","    contacts = []\n","    for c in soup.select(\"#page_contacts .one_fifth\"):\n","\n","        name = c.find(\"h3\")\n","        role = c.find(\"h4\")\n","        phone_raw = c.find(\"p\")\n","        email_tag = c.find(\"a\", href=lambda h: h and \"mailto\" in h)\n","\n","        phone = None\n","        if phone_raw:\n","            phone = clean_text(phone_raw.get_text()).split()[0]\n","\n","        contacts.append({\n","            \"name\": clean_text(name.get_text(separator=\" \")) if name else None,\n","            \"role\": clean_text(role.get_text(separator=\" \")) if role else None,\n","            \"phone\": phone,\n","            \"email\": email_tag[\"href\"].replace(\"mailto:\", \"\") if email_tag else None\n","        })\n","\n","    # SECTION PAR H2\n","    for h2 in soup.select(\".standard_wrapper.cat_wrapper h2\"):\n","\n","        title = clean_text(h2.get_text())\n","        section = {\"h2\": title, \"content\": []}\n","\n","        if \"procÃ©dure\" in title.lower() or \"procÃ©dure\" in title.lower():\n","            section[\"content\"] = subrubrics\n","\n","        elif \"contact\" in title.lower():\n","            section[\"content\"] = contacts\n","\n","        data[\"sections\"].append(section)\n","\n","    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n","    # SCRAPER DES SOUS-PAGES\n","    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n","    for item in subrubrics:\n","        if item[\"url\"]:\n","            print(\"â†’ Scraping sous-page :\", item[\"url\"])\n","            item[\"child_page\"] = scrape_child_page(item[\"url\"])\n","\n","    # FULL TEXT GLOBAL\n","    html = trafilatura.fetch_url(url)\n","    data[\"full_text\"] = trafilatura.extract(html) if html else None\n","\n","    return data\n","\n","\n","# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n","# EXECUTION\n","# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n","URL = \"https://www.esilv.fr/admissions/\"\n","result = scrape_admissions(URL)\n","\n","with open(\"admissions_full.json\", \"w\", encoding=\"utf-8\") as f:\n","    json.dump(result, f, indent=2, ensure_ascii=False)\n","\n","print(\"âœ” admissions_full.json crÃ©Ã© avec succÃ¨s !\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"bqYDElG44jkm","executionInfo":{"status":"ok","timestamp":1764776638699,"user_tz":-60,"elapsed":13060,"user":{"displayName":"Lisa Naccache","userId":"13815877186016026394"}},"outputId":"c1ac6097-57f2-44a0-e06c-ff9d80d2a00e"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["â†’ Scraping sous-page : https://www.esilv.fr/admissions/concours-avenir-plus/\n","â†’ Scraping sous-page : https://www.esilv.fr/admissions/concours-avenir-prepas/\n","â†’ Scraping sous-page : https://www.esilv.fr/admissions/concours-avenir/\n","â†’ Scraping sous-page : https://www.esilv.fr/admissions/candidats-alternance/\n","â†’ Scraping sous-page : https://www.esilv.fr/admissions/concours-avenir-bachelors/\n","â†’ Scraping sous-page : https://www.esilv.fr/admissions/candidats-etrangers/\n","â†’ Scraping sous-page : https://www.esilv.fr/admissions/logement/\n","â†’ Scraping sous-page : https://www.esilv.fr/admissions/rencontrez-nous/\n","â†’ Scraping sous-page : https://www.esilv.fr/admissions/tarifs-et-financement/\n","âœ” admissions_full.json crÃ©Ã© avec succÃ¨s !\n"]}]},{"cell_type":"code","source":["import requests\n","from bs4 import BeautifulSoup\n","from urllib.parse import urljoin\n","import trafilatura\n","import json\n","import re\n","\n","\n","def clean_text(t):\n","    if not t:\n","        return None\n","    t = re.sub(r\"\\s+\", \" \", t)\n","    return t.strip()\n","\n","\n","# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n","# PARSEUR DES BLOCS \"one_third\" AVEC TITRE + LISTE UL\n","# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n","def parse_titled_list(div):\n","    \"\"\"\n","    Ex :\n","    <div class='one_third'>\n","        <strong>ESILV Paris (FISE) :</strong>\n","        <ul><li>MP : 63 places</li>...</ul>\n","    </div>\n","    \"\"\"\n","    title_tag = div.find(\"strong\")\n","    title = clean_text(title_tag.get_text()) if title_tag else None\n","\n","    ul = div.find(\"ul\")\n","    if not ul:\n","        return None\n","\n","    items = [clean_text(li.get_text()) for li in ul.find_all(\"li\")]\n","\n","    return {\n","        \"title\": title,\n","        \"items\": items\n","    }\n","\n","\n","# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n","# SCRAPER STANDARD POUR PAGE ENFANT\n","# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n","def scrape_child_page(url):\n","    r = requests.get(url)\n","    soup = BeautifulSoup(r.text, \"html.parser\")\n","\n","    result = {\n","        \"url\": url,\n","        \"title\": None,\n","        \"intro\": None,\n","        \"blocks\": []\n","    }\n","\n","    # h2 principal\n","    h2 = soup.select_one(\".standard_wrapper.cat_wrapper h2\")\n","    if h2:\n","        result[\"title\"] = clean_text(h2.get_text())\n","\n","    # h4 introductif\n","    intro = soup.select_one(\".standard_wrapper.cat_wrapper > h4\")\n","    if intro:\n","        result[\"intro\"] = clean_text(intro.get_text())\n","\n","    # PARSE : one_half + one_third\n","    for section in soup.select(\".one_half, .one_half.last, .one_third\"):\n","\n","        # Cas spÃ©cial : bloc one_third (titre + ul)\n","        titled_list = parse_titled_list(section)\n","        if titled_list:\n","            result[\"blocks\"].append({\n","                \"type\": \"titled_list\",\n","                \"title\": titled_list[\"title\"],\n","                \"items\": titled_list[\"items\"]\n","            })\n","            continue\n","\n","        # Sinon bloc normal\n","        block = {\n","            \"h3\": None,\n","            \"h4\": None,\n","            \"text\": [],\n","            \"lists\": []\n","        }\n","\n","        h3 = section.find(\"h3\")\n","        h4 = section.find(\"h4\")\n","\n","        if h3:\n","            block[\"h3\"] = clean_text(h3.get_text())\n","        if h4:\n","            block[\"h4\"] = clean_text(h4.get_text())\n","\n","        # Paragraphes\n","        for p in section.find_all(\"p\"):\n","            txt = clean_text(p.get_text())\n","            if txt:\n","                block[\"text\"].append(txt)\n","\n","        # Listes\n","        for ul in section.find_all(\"ul\"):\n","            items = [clean_text(li.get_text()) for li in ul.find_all(\"li\")]\n","            block[\"lists\"].append(items)\n","\n","        if block[\"h3\"] or block[\"h4\"] or block[\"text\"] or block[\"lists\"]:\n","            result[\"blocks\"].append(block)\n","\n","    # Texte global\n","    html = trafilatura.fetch_url(url)\n","    result[\"full_text\"] = trafilatura.extract(html) if html else None\n","\n","    return result\n","\n","\n","\n","# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n","# SCRAPER ADMISSIONS (PAGE PARENTE)\n","# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n","def scrape_admissions(url):\n","\n","    r = requests.get(url)\n","    soup = BeautifulSoup(r.text, \"html.parser\")\n","\n","    data = {\n","        \"rubric\": \"Admissions\",\n","        \"intro\": None,\n","        \"sections\": []\n","    }\n","\n","    # intro\n","    intro_h4 = soup.select_one(\".standard_wrapper.cat_wrapper > h4\")\n","    if intro_h4:\n","        data[\"intro\"] = clean_text(intro_h4.get_text())\n","\n","    # SUBRUBRICS\n","    subrubrics = []\n","    for card in soup.select(\".post_wrapper.one_half\"):\n","\n","        title = card.select_one(\".post_header.half h3 a\")\n","        content = card.select_one(\".post_excerpt.half p\")\n","        btn = card.select_one(\".post_excerpt.half a[href]\")\n","\n","        subrubrics.append({\n","            \"title\": clean_text(title.get_text()) if title else None,\n","            \"content\": clean_text(content.get_text()) if content else None,\n","            \"url\": urljoin(url, btn[\"href\"]) if btn else None,\n","            \"child_page\": None\n","        })\n","\n","    # CONTACTS\n","    contacts = []\n","    for c in soup.select(\"#page_contacts .one_fifth\"):\n","\n","        name = c.find(\"h3\")\n","        role = c.find(\"h4\")\n","        phone_raw = c.find(\"p\")\n","        email_tag = c.find(\"a\", href=lambda h: h and \"mailto\" in h)\n","\n","        phone = None\n","        if phone_raw:\n","            phone = clean_text(phone_raw.get_text()).split()[0]\n","\n","        contacts.append({\n","            \"name\": clean_text(name.get_text(separator=\" \")) if name else None,\n","            \"role\": clean_text(role.get_text(separator=\" \")) if role else None,\n","            \"phone\": phone,\n","            \"email\": email_tag[\"href\"].replace(\"mailto:\", \"\") if email_tag else None\n","        })\n","\n","    # SECTIONS : rÃ©cupÃ©rer aussi le texte entre H2 et H2 suivant\n","    for h2 in soup.select(\".standard_wrapper.cat_wrapper h2\"):\n","\n","        title = clean_text(h2.get_text())\n","        section = {\"h2\": title, \"h3\": None, \"content\": []}\n","\n","        # si un h3 juste aprÃ¨s\n","        nxt = h2.find_next_sibling()\n","        if nxt and nxt.name == \"h3\":\n","            section[\"h3\"] = clean_text(nxt.get_text())\n","\n","        # Cas 1 : procÃ©dures â†’ subrubrics\n","        if \"procÃ©dure\" in title.lower():\n","            section[\"content\"] = subrubrics\n","\n","        # Cas 2 : contacts\n","        elif \"contact\" in title.lower():\n","            section[\"content\"] = contacts\n","\n","        # Cas 3 : texte brut entre sections\n","        else:\n","            cur = h2.find_next_sibling()\n","            while cur and cur.name not in [\"h2\", \"hr\"]:\n","                if cur.name == \"p\":\n","                    txt = clean_text(cur.get_text())\n","                    if txt:\n","                        section[\"content\"].append(txt)\n","                cur = cur.find_next_sibling()\n","\n","        data[\"sections\"].append(section)\n","\n","    # SCRAPER CHILDS\n","    for item in subrubrics:\n","        if item[\"url\"]:\n","            print(\"Scraping child:\", item[\"url\"])\n","            item[\"child_page\"] = scrape_child_page(item[\"url\"])\n","\n","    # texte complet\n","    html = trafilatura.fetch_url(url)\n","    data[\"full_text\"] = trafilatura.extract(html) if html else None\n","\n","    return data\n","\n","\n","# RUN\n","URL = \"https://www.esilv.fr/admissions/\"\n","result = scrape_admissions(URL)\n","\n","with open(\"admissions_withchild.json\", \"w\", encoding=\"utf-8\") as f:\n","    json.dump(result, f, indent=2, ensure_ascii=False)\n","\n","print(\"âœ” FINI !\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"IdYZ1LvIAngM","executionInfo":{"status":"ok","timestamp":1764779225607,"user_tz":-60,"elapsed":13294,"user":{"displayName":"Lisa Naccache","userId":"13815877186016026394"}},"outputId":"55042ac1-f9f1-44ea-89a3-18e37cb3db50"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Scraping child: https://www.esilv.fr/admissions/concours-avenir-plus/\n","Scraping child: https://www.esilv.fr/admissions/concours-avenir-prepas/\n","Scraping child: https://www.esilv.fr/admissions/concours-avenir/\n","Scraping child: https://www.esilv.fr/admissions/candidats-alternance/\n","Scraping child: https://www.esilv.fr/admissions/concours-avenir-bachelors/\n","Scraping child: https://www.esilv.fr/admissions/candidats-etrangers/\n","Scraping child: https://www.esilv.fr/admissions/logement/\n","Scraping child: https://www.esilv.fr/admissions/rencontrez-nous/\n","Scraping child: https://www.esilv.fr/admissions/tarifs-et-financement/\n","âœ” FINI !\n"]}]},{"cell_type":"code","source":["# ============================================================\n","# UTILITAIRES\n","# ============================================================\n","\n","def clean_text(t):\n","    if not t:\n","        return None\n","    t = re.sub(r\"\\s+\", \" \", t)\n","    return t.strip()\n","\n","\n","def safe_text(tag):\n","    try:\n","        return clean_text(tag.get_text())\n","    except:\n","        return None\n","\n","\n","def fetch(url):\n","    try:\n","        r = requests.get(url, timeout=10)\n","        r.raise_for_status()\n","        return r.text\n","    except Exception as e:\n","        print(f\"âš ï¸ ERROR fetching {url} â†’ {e}\")\n","        return None\n","\n","\n","# ============================================================\n","# TITLED LIST (\"one_third\")\n","# ============================================================\n","\n","def parse_titled_list(div):\n","    try:\n","        title_tag = div.find(\"strong\")\n","        title = safe_text(title_tag) if title_tag else None\n","\n","        ul = div.find(\"ul\")\n","        if not ul:\n","            return None\n","\n","        items = [safe_text(li) for li in ul.find_all(\"li\")]\n","        items = [i for i in items if i]\n","\n","        return {\"title\": title, \"items\": items}\n","    except:\n","        return None\n","\n","\n","# ============================================================\n","# PARSEUR DE SOUS-PAGE (CHILD PAGE)\n","# ============================================================\n","\n","def scrape_child_page(url):\n","    print(f\"    â†³ Child page: {url}\")\n","\n","    html = fetch(url)\n","    if not html:\n","        return None\n","\n","    soup = BeautifulSoup(html, \"html.parser\")\n","\n","    data = {\n","        \"url\": url,\n","        \"title\": None,\n","        \"intro\": None,\n","        \"blocks\": [],\n","        \"full_text\": None\n","    }\n","\n","    h2 = soup.select_one(\".standard_wrapper.cat_wrapper h2\")\n","    if h2:\n","        data[\"title\"] = safe_text(h2)\n","\n","    intro = soup.select_one(\".standard_wrapper.cat_wrapper > h4\")\n","    if intro:\n","        data[\"intro\"] = safe_text(intro)\n","\n","    # Blocs structurÃ©s\n","    for section in soup.select(\".one_half, .one_third, .one_half.last\"):\n","\n","        titled = parse_titled_list(section)\n","        if titled:\n","            data[\"blocks\"].append({\"type\": \"titled_list\", **titled})\n","            continue\n","\n","        block = {\"h3\": None, \"h4\": None, \"text\": [], \"lists\": []}\n","\n","        h3 = section.find(\"h3\")\n","        h4 = section.find(\"h4\")\n","        if h3: block[\"h3\"] = safe_text(h3)\n","        if h4: block[\"h4\"] = safe_text(h4)\n","\n","        for p in section.find_all(\"p\"):\n","            txt = safe_text(p)\n","            if txt:\n","                block[\"text\"].append(txt)\n","\n","        for ul in section.find_all(\"ul\"):\n","            li = [safe_text(x) for x in ul.find_all(\"li\")]\n","            block[\"lists\"].append([x for x in li if x])\n","\n","        if block[\"h3\"] or block[\"h4\"] or block[\"text\"] or block[\"lists\"]:\n","            data[\"blocks\"].append(block)\n","\n","    # Texte complet\n","    raw = trafilatura.fetch_url(url)\n","    data[\"full_text\"] = trafilatura.extract(raw) if raw else None\n","\n","    return data\n","\n","\n","# ============================================================\n","# PARSEUR GÃ‰NÃ‰RIQUE (valable pour TOUTES les rubriques)\n","# ============================================================\n","\n","def scrape_generic_rubric(name, url):\n","    print(f\"\\nâ†’ Scraping rubric: {name} ({url})\")\n","\n","    html = fetch(url)\n","    if not html:\n","        return None\n","    soup = BeautifulSoup(html, \"html.parser\")\n","\n","    rubric = {\n","        \"rubric\": name,\n","        \"intro\": None,\n","        \"sections\": [],\n","        \"child_pages\": [],\n","        \"full_text\": None\n","    }\n","\n","    # Intro H4\n","    intro = soup.select_one(\".standard_wrapper.cat_wrapper > h4\")\n","    if intro:\n","        rubric[\"intro\"] = safe_text(intro)\n","\n","    # Sous-pages dÃ©tectÃ©es (h3 â†’ liens)\n","    all_child_links = set()\n","\n","    for a in soup.select(\".post_header.half a[href]\"):\n","        link = urljoin(url, a[\"href\"])\n","        all_child_links.add(link)\n","\n","    for a in soup.select(\".post_excerpt a[href]\"):\n","        link = urljoin(url, a[\"href\"])\n","        all_child_links.add(link)\n","\n","    # Sections H2\n","    for h2 in soup.select(\".standard_wrapper.cat_wrapper h2\"):\n","\n","        sec = {\n","            \"h2\": safe_text(h2),\n","            \"h3\": None,\n","            \"content\": [],\n","            \"subrubrics\": [],\n","            \"contacts\": []\n","        }\n","\n","        # H3 immÃ©diatement aprÃ¨s\n","        nxt = h2.find_next_sibling()\n","        if nxt and nxt.name == \"h3\":\n","            sec[\"h3\"] = safe_text(nxt)\n","\n","        # Contenu textuel entre H2 et H2 suivant\n","        cur = h2.find_next_sibling()\n","        while cur and cur.name not in [\"h2\", \"hr\"]:\n","\n","            # Paragraphes\n","            if cur.name == \"p\":\n","                txt = safe_text(cur)\n","                if txt:\n","                    sec[\"content\"].append(txt)\n","\n","            # Divs contenant du texte\n","            if cur.name == \"div\":\n","                txt = safe_text(cur)\n","                if txt and len(txt) > 3:\n","                    sec[\"content\"].append(txt)\n","\n","            cur = cur.find_next_sibling()\n","\n","        rubric[\"sections\"].append(sec)\n","\n","    # Child pages (mÃªme logique que Admissions)\n","    for link in all_child_links:\n","        cp = scrape_child_page(link)\n","        rubric[\"child_pages\"].append(cp)\n","\n","    # Texte complet\n","    raw = trafilatura.fetch_url(url)\n","    rubric[\"full_text\"] = trafilatura.extract(raw) if raw else None\n","\n","    return rubric\n","\n","\n","# ============================================================\n","# MANAGER PRINCIPAL\n","# ============================================================\n","\n","START_URLS = {\n","    \"lecole\": \"https://www.esilv.fr/lecole/\",\n","    \"admissions\": \"https://www.esilv.fr/admissions/\",\n","    \"formations\": \"https://www.esilv.fr/formations/\",\n","    \"entreprises-debouches\": \"https://www.esilv.fr/entreprises-debouches/\",\n","    \"recherche\": \"https://www.esilv.fr/recherche/\",\n","    \"international\": \"https://www.esilv.fr/international/\",\n","}\n","\n","\n","OUTPUT_DIR = \"scraping_esilv2\"\n","os.makedirs(OUTPUT_DIR, exist_ok=True)\n","\n","\n","def scrape_all():\n","    for name, url in START_URLS.items():\n","        try:\n","            print(\"\\n============================\")\n","            print(f\" SCRAPING {name}\")\n","            print(\"============================\")\n","\n","            data = scrape_generic_rubric(name, url)\n","\n","            with open(f\"{OUTPUT_DIR}/{name}.json\", \"w\", encoding=\"utf-8\") as f:\n","                json.dump(data, f, indent=2, ensure_ascii=False)\n","\n","            print(f\"âœ” Export â†’ {OUTPUT_DIR}/{name}.json\")\n","\n","        except Exception as e:\n","            print(f\"âŒ Error scraping {name}: {e}\")\n","            traceback.print_exc()\n","\n","\n","# LANCER\n","scrape_all()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"cU7Sa2Z5EztA","executionInfo":{"status":"ok","timestamp":1764784150833,"user_tz":-60,"elapsed":13593,"user":{"displayName":"Lisa Naccache","userId":"13815877186016026394"}},"outputId":"19260737-d110-459e-d940-90c432367e9c"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","============================\n"," SCRAPING admissions\n","============================\n","\n","â†’ Scraping rubric: admissions (https://www.esilv.fr/admissions/)\n","    â†³ Child page: https://www.esilv.fr/admissions/candidats-etrangers/\n","    â†³ Child page: https://www.esilv.fr/admissions/rencontrez-nous/\n","    â†³ Child page: https://www.esilv.fr/admissions/concours-avenir-bachelors/\n","    â†³ Child page: https://www.esilv.fr/admissions/concours-avenir/\n","    â†³ Child page: https://www.esilv.fr/admissions/concours-avenir-prepas/\n","    â†³ Child page: https://www.esilv.fr/admissions/concours-avenir-plus/\n","    â†³ Child page: https://www.esilv.fr/admissions/tarifs-et-financement/\n","    â†³ Child page: https://www.esilv.fr/admissions/logement/\n","    â†³ Child page: https://www.esilv.fr/admissions/candidats-alternance/\n","âœ” Export â†’ scraping_esilv2/admissions.json\n"]}]},{"cell_type":"markdown","source":["## Version officiel"],"metadata":{"id":"Mi1wUdLiZXCk"}},{"cell_type":"code","source":["# ============================================================\n","# UTILITAIRES\n","# ============================================================\n","\n","def clean_text(t):\n","    if not t:\n","        return None\n","    t = re.sub(r\"\\s+\", \" \", t)\n","    return t.strip()\n","\n","\n","def safe_text(tag):\n","    try:\n","        return clean_text(tag.get_text())\n","    except:\n","        return None\n","\n","\n","def fetch(url):\n","    try:\n","        r = requests.get(url, timeout=10)\n","        r.raise_for_status()\n","        return r.text\n","    except Exception as e:\n","        print(f\"âš ï¸ ERROR fetching {url} â†’ {e}\")\n","        return None\n","\n","\n","# ============================================================\n","# TITLED LIST (\"one_third\")\n","# ============================================================\n","\n","def parse_titled_list(div):\n","    try:\n","        title_tag = div.find(\"strong\")\n","        title = safe_text(title_tag) if title_tag else None\n","\n","        ul = div.find(\"ul\")\n","        if not ul:\n","            return None\n","\n","        items = [safe_text(li) for li in ul.find_all(\"li\")]\n","        items = [i for i in items if i]\n","\n","        return {\"title\": title, \"items\": items}\n","    except:\n","        return None\n","\n","\n","# ============================================================\n","# PARSEUR DE SOUS-PAGE (CHILD PAGE)\n","# ============================================================\n","\n","def scrape_child_page(url):\n","    print(f\"    â†³ Child page: {url}\")\n","\n","    html = fetch(url)\n","    if not html:\n","        return None\n","\n","    soup = BeautifulSoup(html, \"html.parser\")\n","\n","    data = {\n","        \"url\": url,\n","        \"title\": None,\n","        \"intro\": None,\n","        \"blocks\": [],\n","        \"full_text\": None\n","    }\n","\n","    h2 = soup.select_one(\".standard_wrapper.cat_wrapper h2\")\n","    if h2:\n","        data[\"title\"] = safe_text(h2)\n","\n","    intro = soup.select_one(\".standard_wrapper.cat_wrapper > h4\")\n","    if intro:\n","        data[\"intro\"] = safe_text(intro)\n","\n","    # Blocs structurÃ©s\n","    for section in soup.select(\".one_half, .one_third, .one_half.last\"):\n","\n","        titled = parse_titled_list(section)\n","        if titled:\n","            data[\"blocks\"].append({\"type\": \"titled_list\", **titled})\n","            continue\n","\n","        block = {\"h3\": None, \"h4\": None, \"text\": [], \"lists\": []}\n","\n","        h3 = section.find(\"h3\")\n","        h4 = section.find(\"h4\")\n","        if h3: block[\"h3\"] = safe_text(h3)\n","        if h4: block[\"h4\"] = safe_text(h4)\n","\n","        for p in section.find_all(\"p\"):\n","            txt = safe_text(p)\n","            if txt:\n","                block[\"text\"].append(txt)\n","\n","        for ul in section.find_all(\"ul\"):\n","            li = [safe_text(x) for x in ul.find_all(\"li\")]\n","            block[\"lists\"].append([x for x in li if x])\n","\n","        if block[\"h3\"] or block[\"h4\"] or block[\"text\"] or block[\"lists\"]:\n","            data[\"blocks\"].append(block)\n","\n","    # Texte complet\n","    raw = trafilatura.fetch_url(url)\n","    data[\"full_text\"] = trafilatura.extract(raw) if raw else None\n","\n","    return data\n","\n","\n","# ============================================================\n","# PARSEUR GÃ‰NÃ‰RIQUE (valable pour TOUTES les rubriques)\n","# ============================================================\n","\n","def scrape_generic_rubric(name, url):\n","    print(f\"\\nâ†’ Scraping rubric: {name} ({url})\")\n","\n","    html = fetch(url)\n","    if not html:\n","        return None\n","    soup = BeautifulSoup(html, \"html.parser\")\n","\n","    rubric = {\n","        \"rubric\": name,\n","        \"intro\": None,\n","        \"sections\": [],\n","        \"child_pages\": [],\n","        \"full_text\": None\n","    }\n","\n","    # Intro H4\n","    intro = soup.select_one(\".standard_wrapper.cat_wrapper > h4\")\n","    if intro:\n","        rubric[\"intro\"] = safe_text(intro)\n","\n","    # Sous-pages dÃ©tectÃ©es (h3 â†’ liens)\n","    all_child_links = set()\n","\n","    for a in soup.select(\".post_header.half a[href]\"):\n","        link = urljoin(url, a[\"href\"])\n","        all_child_links.add(link)\n","\n","    for a in soup.select(\".post_excerpt a[href]\"):\n","        link = urljoin(url, a[\"href\"])\n","        all_child_links.add(link)\n","\n","    # Sections H2\n","    for h2 in soup.select(\".standard_wrapper.cat_wrapper h2\"):\n","\n","        sec = {\n","            \"h2\": safe_text(h2),\n","            \"h3\": None,\n","            \"content\": [],\n","            \"subrubrics\": [],\n","            \"contacts\": []\n","        }\n","\n","        # H3 immÃ©diatement aprÃ¨s\n","        nxt = h2.find_next_sibling()\n","        if nxt and nxt.name == \"h3\":\n","            sec[\"h3\"] = safe_text(nxt)\n","\n","        # Contenu textuel entre H2 et H2 suivant\n","        cur = h2.find_next_sibling()\n","        while cur and cur.name not in [\"h2\", \"hr\"]:\n","\n","            # Paragraphes\n","            if cur.name == \"p\":\n","                txt = safe_text(cur)\n","                if txt:\n","                    sec[\"content\"].append(txt)\n","\n","            # Divs contenant du texte\n","            if cur.name == \"div\":\n","                txt = safe_text(cur)\n","                if txt and len(txt) > 3:\n","                    sec[\"content\"].append(txt)\n","\n","            cur = cur.find_next_sibling()\n","\n","        rubric[\"sections\"].append(sec)\n","\n","    # Child pages (mÃªme logique que Admissions)\n","    for link in all_child_links:\n","        cp = scrape_child_page(link)\n","        rubric[\"child_pages\"].append(cp)\n","\n","    # Texte complet\n","    raw = trafilatura.fetch_url(url)\n","    rubric[\"full_text\"] = trafilatura.extract(raw) if raw else None\n","\n","    return rubric\n","\n","\n","# ============================================================\n","# MANAGER PRINCIPAL\n","# ============================================================\n","\n","START_URLS = {\n","    \"lecole\": \"https://www.esilv.fr/lecole/\",\n","    \"admissions\": \"https://www.esilv.fr/admissions/\",\n","    \"formations\": \"https://www.esilv.fr/formations/\",\n","    \"entreprises-debouches\": \"https://www.esilv.fr/entreprises-debouches/\",\n","    \"recherche\": \"https://www.esilv.fr/recherche/\",\n","    \"international\": \"https://www.esilv.fr/international/\",\n","}\n","\n","OUTPUT_DIR = \"scraping_esilv3\"\n","os.makedirs(OUTPUT_DIR, exist_ok=True)\n","\n","\n","def scrape_all():\n","    for name, url in START_URLS.items():\n","        try:\n","            print(\"\\n============================\")\n","            print(f\" SCRAPING {name}\")\n","            print(\"============================\")\n","\n","            data = scrape_generic_rubric(name, url)\n","\n","            with open(f\"{OUTPUT_DIR}/{name}.json\", \"w\", encoding=\"utf-8\") as f:\n","                json.dump(data, f, indent=2, ensure_ascii=False)\n","\n","            print(f\"âœ” Export â†’ {OUTPUT_DIR}/{name}.json\")\n","\n","        except Exception as e:\n","            print(f\"âŒ Error scraping {name}: {e}\")\n","            traceback.print_exc()\n","\n","\n","# LANCER\n","scrape_all()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"uV_baeDabK1Z","executionInfo":{"status":"ok","timestamp":1764786850802,"user_tz":-60,"elapsed":16501,"user":{"displayName":"Lisa Naccache","userId":"13815877186016026394"}},"outputId":"3acc3c88-a68d-4371-e003-76379c320ae6"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","============================\n"," SCRAPING admissions\n","============================\n","\n","â†’ Scraping rubric: admissions (https://www.esilv.fr/admissions/)\n","    â†³ Child page: https://www.esilv.fr/admissions/concours-avenir-bachelors/\n","    â†³ Child page: https://www.esilv.fr/admissions/candidats-alternance/\n","    â†³ Child page: https://www.esilv.fr/admissions/rencontrez-nous/\n","    â†³ Child page: https://www.esilv.fr/admissions/tarifs-et-financement/\n","    â†³ Child page: https://www.esilv.fr/admissions/candidats-etrangers/\n","    â†³ Child page: https://www.esilv.fr/admissions/logement/\n","    â†³ Child page: https://www.esilv.fr/admissions/concours-avenir/\n","    â†³ Child page: https://www.esilv.fr/admissions/concours-avenir-prepas/\n","    â†³ Child page: https://www.esilv.fr/admissions/concours-avenir-plus/\n","âœ” Export â†’ scraping_esilv3/admissions.json\n"]}]}]}