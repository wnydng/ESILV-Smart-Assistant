{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2026-01-06T13:13:14.639021Z",
     "start_time": "2026-01-06T13:13:14.634728Z"
    }
   },
   "source": [
    "import json\n",
    "import re\n",
    "import hashlib\n",
    "from pathlib import Path\n",
    "from typing import Dict, Any, List, Tuple\n"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-06T13:13:36.605089Z",
     "start_time": "2026-01-06T13:13:36.589400Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# ----------------------------\n",
    "# TEXT CLEANING (safe, no info loss)\n",
    "# ----------------------------\n",
    "FOOTER_RE = re.compile(r\"(I\\s*DE\\s*VINCI\\s*ENGINEERING\\s*SCHOOL\\s*ESILV\\s*\\d+)\", re.IGNORECASE)\n",
    "POLE_RE = re.compile(r\"(P[oô]le\\s*L[eé]onard\\s*de\\s*Vinci)\", re.IGNORECASE)\n",
    "\n",
    "def normalize_spaces(s: str) -> str:\n",
    "    # Keep content, just normalize whitespace\n",
    "    s = s.replace(\"\\u00a0\", \" \")  # NBSP\n",
    "    s = re.sub(r\"[ \\t]+\", \" \", s)\n",
    "    s = re.sub(r\"\\s*\\n\\s*\", \"\\n\", s)   # clean around newlines\n",
    "    s = re.sub(r\"\\n{3,}\", \"\\n\\n\", s)   # limit empty lines\n",
    "    return s.strip()\n",
    "\n",
    "def extract_footer_candidates(text: str) -> Tuple[str, List[str]]:\n",
    "    \"\"\"\n",
    "    Do NOT remove info: we keep text_raw untouched in output.\n",
    "    Here we only \"detect\" footer candidates and also produce a 'text_wo_footer_norm'\n",
    "    for better embeddings if you want it later.\n",
    "    \"\"\"\n",
    "    footers = FOOTER_RE.findall(text)\n",
    "    # Build a version where footer strings are removed ONLY in the derived field.\n",
    "    text_wo = text\n",
    "    for f in set(footers):\n",
    "        text_wo = re.sub(re.escape(f), \" \", text_wo, flags=re.IGNORECASE)\n",
    "    text_wo = normalize_spaces(text_wo)\n",
    "    return text_wo, sorted(set(footers))\n",
    "\n",
    "def basic_stats(s: str) -> Dict[str, int]:\n",
    "    words = re.findall(r\"\\b\\w+\\b\", s, flags=re.UNICODE)\n",
    "    return {\n",
    "        \"chars\": len(s),\n",
    "        \"words\": len(words),\n",
    "        \"lines\": s.count(\"\\n\") + (1 if s else 0),\n",
    "    }\n",
    "\n",
    "def stable_id(*parts: str) -> str:\n",
    "    h = hashlib.sha1(\"||\".join(parts).encode(\"utf-8\")).hexdigest()\n",
    "    return h[:16]"
   ],
   "id": "30f57dfebb5f9a62",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-06T13:14:18.569303Z",
     "start_time": "2026-01-06T13:14:18.552430Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# ----------------------------\n",
    "# CHUNKING (no info loss)\n",
    "# - Keeps raw; chunks use normalized (optionally without detected footer)\n",
    "# ----------------------------\n",
    "def chunk_text(text: str, max_chars: int = 1200, overlap: int = 150) -> List[str]:\n",
    "    \"\"\"\n",
    "    Chunk by paragraphs/sentences-ish without deleting content.\n",
    "    \"\"\"\n",
    "    if not text:\n",
    "        return []\n",
    "\n",
    "    # Split on blank lines first (paragraphs)\n",
    "    paras = re.split(r\"\\n\\s*\\n\", text)\n",
    "    paras = [p.strip() for p in paras if p.strip()]\n",
    "\n",
    "    chunks = []\n",
    "    buf = \"\"\n",
    "\n",
    "    def flush_buf(b: str):\n",
    "        if b.strip():\n",
    "            chunks.append(b.strip())\n",
    "\n",
    "    for p in paras:\n",
    "        if len(buf) + len(p) + 2 <= max_chars:\n",
    "            buf = (buf + \"\\n\\n\" + p).strip() if buf else p\n",
    "        else:\n",
    "            # If paragraph itself too large, split further\n",
    "            if buf:\n",
    "                flush_buf(buf)\n",
    "                buf = \"\"\n",
    "\n",
    "            if len(p) <= max_chars:\n",
    "                buf = p\n",
    "            else:\n",
    "                # Split long paragraph on sentences / punctuation (fallback: hard split)\n",
    "                parts = re.split(r\"(?<=[\\.\\!\\?\\:;])\\s+\", p)\n",
    "                tmp = \"\"\n",
    "                for part in parts:\n",
    "                    part = part.strip()\n",
    "                    if not part:\n",
    "                        continue\n",
    "                    if len(tmp) + len(part) + 1 <= max_chars:\n",
    "                        tmp = (tmp + \" \" + part).strip() if tmp else part\n",
    "                    else:\n",
    "                        flush_buf(tmp)\n",
    "                        tmp = part\n",
    "                flush_buf(tmp)\n",
    "\n",
    "    flush_buf(buf)\n",
    "\n",
    "    # Add overlap (character-based) to keep context continuity\n",
    "    if overlap > 0 and len(chunks) > 1:\n",
    "        overlapped = [chunks[0]]\n",
    "        for i in range(1, len(chunks)):\n",
    "            prev = overlapped[-1]\n",
    "            tail = prev[-overlap:] if len(prev) > overlap else prev\n",
    "            overlapped.append((tail + \"\\n\" + chunks[i]).strip())\n",
    "        chunks = overlapped\n",
    "\n",
    "    return chunks"
   ],
   "id": "1951131d831971c1",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-06T13:14:32.169035Z",
     "start_time": "2026-01-06T13:14:32.150935Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# ----------------------------\n",
    "# MAIN TRANSFORM\n",
    "# ----------------------------\n",
    "def improve_pdf_json(data: Dict[str, Any],\n",
    "                     max_chars_per_chunk: int = 1200,\n",
    "                     overlap: int = 150) -> Dict[str, Any]:\n",
    "\n",
    "    out = {\n",
    "        \"rubric\": data.get(\"rubric\", \"\"),\n",
    "        \"documents\": []\n",
    "    }\n",
    "\n",
    "    for doc in data.get(\"documents\", []):\n",
    "        pdf_name = doc.get(\"pdf_name\", \"\")\n",
    "        id_pdf = doc.get(\"id_pdf\", doc.get(\"pdf_id\", doc.get(\"pdf\", \"\")))\n",
    "\n",
    "        new_doc = {\n",
    "            \"pdf_name\": pdf_name,\n",
    "            \"id_pdf\": id_pdf,\n",
    "            # keep original pages, but enrich them\n",
    "            \"pages\": [],\n",
    "            # add chunks for embedding\n",
    "            \"chunks\": []\n",
    "        }\n",
    "\n",
    "        for page_obj in doc.get(\"pages\", []):\n",
    "            page_num = str(page_obj.get(\"page\", \"\"))\n",
    "            raw = page_obj.get(\"text\", \"\") or \"\"\n",
    "\n",
    "            raw_norm = normalize_spaces(raw)\n",
    "            wo_footer_norm, footer_hits = extract_footer_candidates(raw_norm)\n",
    "\n",
    "            page_enriched = {\n",
    "                \"page\": page_obj.get(\"page\"),\n",
    "                \"text_raw\": raw,                 # EXACT original OCR\n",
    "                \"text_norm\": raw_norm,           # whitespace-normalized\n",
    "                \"text_norm_wo_footer\": wo_footer_norm,  # derived field (optional for embedding)\n",
    "                \"footer_candidates\": footer_hits,        # detected, not removed from raw\n",
    "                \"stats_raw\": basic_stats(raw),\n",
    "                \"stats_norm\": basic_stats(raw_norm),\n",
    "            }\n",
    "            new_doc[\"pages\"].append(page_enriched)\n",
    "\n",
    "            # Build embedding input from normalized-without-footer (better retrieval),\n",
    "            # but you still keep the raw in the doc.\n",
    "            # If you prefer, switch to raw_norm (keep footer) for chunks too.\n",
    "            chunks = chunk_text(wo_footer_norm, max_chars=max_chars_per_chunk, overlap=overlap)\n",
    "\n",
    "            for ci, c in enumerate(chunks):\n",
    "                chunk_id = stable_id(pdf_name, str(id_pdf), page_num, str(ci))\n",
    "                new_doc[\"chunks\"].append({\n",
    "                    \"chunk_id\": chunk_id,\n",
    "                    \"page\": page_obj.get(\"page\"),\n",
    "                    \"chunk_index\": ci,\n",
    "                    \"text\": c,\n",
    "                    \"stats\": basic_stats(c),\n",
    "                    # provenance\n",
    "                    \"pdf_name\": pdf_name,\n",
    "                    \"id_pdf\": id_pdf,\n",
    "                    \"rubric\": data.get(\"rubric\", \"\"),\n",
    "                })\n",
    "\n",
    "        out[\"documents\"].append(new_doc)\n",
    "\n",
    "    return out"
   ],
   "id": "5dcf4f71d460c203",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-06T13:16:53.219356Z",
     "start_time": "2026-01-06T13:16:53.206108Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import os\n",
    "os.listdir(\"../../data/scraping_esilv\")"
   ],
   "id": "34ae82e62ee2408b",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['admissions.json',\n",
       " 'entreprises-debouches.json',\n",
       " 'formations.json',\n",
       " 'full_pdfs.json',\n",
       " 'full_pdfs.txt',\n",
       " 'international.json',\n",
       " 'lecole.json',\n",
       " 'recherche.json']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-06T13:17:37.081157Z",
     "start_time": "2026-01-06T13:17:36.865320Z"
    }
   },
   "cell_type": "code",
   "source": [
    "if __name__ == \"__main__\":\n",
    "    # INPUT / OUTPUT\n",
    "    in_path = Path(\"../../data/scraping_esilv/full_pdfs.json\")\n",
    "    out_path = Path(\"../../data/scraping_esilv/full_pdfs_improved.json\")\n",
    "\n",
    "    data = json.loads(in_path.read_text(encoding=\"utf-8\"))\n",
    "    improved = improve_pdf_json(data, max_chars_per_chunk=1200, overlap=150)\n",
    "\n",
    "    out_path.write_text(json.dumps(improved, ensure_ascii=False, indent=2), encoding=\"utf-8\")\n",
    "    print(f\"OK -> {out_path} | docs={len(improved.get('documents', []))}\")"
   ],
   "id": "46af185d4c4a1b2e",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OK -> ..\\..\\data\\scraping_esilv\\full_pdfs_improved.json | docs=7\n"
     ]
    }
   ],
   "execution_count": 9
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
