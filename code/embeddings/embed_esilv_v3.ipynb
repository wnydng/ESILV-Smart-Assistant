{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2026-01-06T13:24:46.491020Z",
     "start_time": "2026-01-06T13:24:44.923653Z"
    }
   },
   "source": [
    "import os, json, re\n",
    "import numpy as np\n",
    "import faiss\n",
    "from ollama import Client"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-06T13:24:48.259996Z",
     "start_time": "2026-01-06T13:24:47.497277Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# -------------------------\n",
    "# Config\n",
    "# -------------------------\n",
    "PDF_JSON_PATH = \"../../data/scraping_esilv/full_pdfs_improved.json\"\n",
    "OUT_DIR = \"vector_store_v3\"\n",
    "INDEX_PATH = os.path.join(OUT_DIR, \"faiss_index.bin\")\n",
    "MAP_PATH = os.path.join(OUT_DIR, \"mapping.json\")\n",
    "\n",
    "EMBED_MODEL = \"mxbai-embed-large\"\n",
    "EMBED_DIM = 1024\n",
    "\n",
    "# Safe chunking\n",
    "MAX_CHARS = 1100\n",
    "MIN_CHARS = 200\n",
    "\n",
    "os.makedirs(OUT_DIR, exist_ok=True)\n",
    "\n",
    "client = Client(host=\"http://localhost:11434\")"
   ],
   "id": "2d1b7f40852de322",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-06T13:24:50.387815Z",
     "start_time": "2026-01-06T13:24:50.371682Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def normalize_vec(v: np.ndarray) -> np.ndarray:\n",
    "    v = v.astype(\"float32\")\n",
    "    v /= (np.linalg.norm(v) + 1e-12)\n",
    "    return v\n",
    "\n",
    "def embed_one(text: str) -> np.ndarray:\n",
    "    # Ollama python client recent versions: client.embed or client.embeddings\n",
    "    # We'll use embed if available, else embeddings.\n",
    "    if hasattr(client, \"embed\"):\n",
    "        resp = client.embed(model=EMBED_MODEL, input=text)\n",
    "        # resp.embeddings is usually list[list[float]]\n",
    "        v = np.array(resp.embeddings[0], dtype=\"float32\")\n",
    "    else:\n",
    "        resp = client.embeddings(model=EMBED_MODEL, prompt=text)\n",
    "        # resp may be dict-like or pydantic\n",
    "        if isinstance(resp, dict) and \"embedding\" in resp:\n",
    "            v = np.array(resp[\"embedding\"], dtype=\"float32\")\n",
    "        else:\n",
    "            v = np.array(resp.embedding, dtype=\"float32\")  # fallback (rare)\n",
    "    return normalize_vec(v)\n",
    "\n",
    "def smart_split(text: str, max_chars: int = MAX_CHARS):\n",
    "    text = re.sub(r\"\\s+\", \" \", text).strip()\n",
    "    if len(text) <= max_chars:\n",
    "        return [text]\n",
    "\n",
    "    parts = []\n",
    "    start = 0\n",
    "    while start < len(text):\n",
    "        end = min(start + max_chars, len(text))\n",
    "        # try split on sentence boundary\n",
    "        cut = text.rfind(\". \", start, end)\n",
    "        if cut == -1 or cut < start + MIN_CHARS:\n",
    "            cut = text.rfind(\" \", start, end)\n",
    "        if cut == -1 or cut < start + MIN_CHARS:\n",
    "            cut = end\n",
    "        part = text[start:cut].strip()\n",
    "        if part:\n",
    "            parts.append(part)\n",
    "        start = cut + 1\n",
    "    return parts"
   ],
   "id": "b54f9234f324764b",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-06T13:24:52.843830Z",
     "start_time": "2026-01-06T13:24:52.821822Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# -------------------------\n",
    "# Load pdf json\n",
    "# -------------------------\n",
    "with open(PDF_JSON_PATH, \"r\", encoding=\"utf-8\") as f:\n",
    "    pdf_root = json.load(f)\n",
    "\n",
    "# pdf_root expected schema:\n",
    "# { \"rubric\": \"...\", \"documents\": [ { \"pdf_name\":..., \"id_pdf\":..., \"pages\":[{page, text_norm_wo_footer...}, ...] } ] }\n",
    "documents = pdf_root.get(\"documents\", [])\n",
    "rubric = pdf_root.get(\"rubric\", \"pdf\")"
   ],
   "id": "c39aae98192ff1e2",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-06T13:24:55.369875Z",
     "start_time": "2026-01-06T13:24:55.361338Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# -------------------------\n",
    "# Create / load FAISS v3\n",
    "# -------------------------\n",
    "if os.path.exists(INDEX_PATH):\n",
    "    index = faiss.read_index(INDEX_PATH)\n",
    "else:\n",
    "    index = faiss.IndexFlatIP(EMBED_DIM)\n",
    "\n",
    "if os.path.exists(MAP_PATH):\n",
    "    with open(MAP_PATH, \"r\", encoding=\"utf-8\") as f:\n",
    "        mapping = json.load(f)\n",
    "else:\n",
    "    mapping = {}\n",
    "\n",
    "def add_vector(vec: np.ndarray, meta: dict):\n",
    "    vec = vec.reshape(1, -1).astype(\"float32\")\n",
    "    idx_id = index.ntotal\n",
    "    index.add(vec)\n",
    "    mapping[str(idx_id)] = meta"
   ],
   "id": "a73a12cf6aa1d245",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-06T13:28:15.534838Z",
     "start_time": "2026-01-06T13:24:58.573214Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# -------------------------\n",
    "# Pipeline\n",
    "# -------------------------\n",
    "added = 0\n",
    "\n",
    "for doc in documents:\n",
    "    pdf_name = doc.get(\"pdf_name\", \"\")\n",
    "    pdf_id = doc.get(\"id_pdf\", doc.get(\"pdf_id\", \"\"))\n",
    "    pages = doc.get(\"pages\", [])\n",
    "\n",
    "    for p in pages:\n",
    "        page_num = p.get(\"page\")\n",
    "        text = p.get(\"text_norm_wo_footer\") or p.get(\"text_norm\") or p.get(\"text_raw\") or \"\"\n",
    "        if not text.strip():\n",
    "            continue\n",
    "\n",
    "        header = f\"SOURCE: pdf\\nPDF_NAME: {pdf_name}\\nPDF_ID: {pdf_id}\\nPAGE: {page_num}\\nRUBRIC: {rubric}\\nTEXT:\\n\"\n",
    "        embed_input = header + text\n",
    "\n",
    "        parts = smart_split(embed_input, MAX_CHARS)\n",
    "\n",
    "        for part_i, part in enumerate(parts):\n",
    "            v = embed_one(part)\n",
    "\n",
    "            meta = {\n",
    "                \"title\": f\"{pdf_name} - page {page_num} - part {part_i+1}/{len(parts)}\",\n",
    "                \"content\": part,  # keep what was embedded (important for debugging)\n",
    "                \"rubric\": rubric,\n",
    "                \"url\": f\"pdf://{pdf_id}#page={page_num}\",\n",
    "                \"source_file\": pdf_id,\n",
    "                \"pdf_name\": pdf_name,\n",
    "                \"page\": page_num,\n",
    "                \"part_index\": part_i,\n",
    "                \"parts_count\": len(parts),\n",
    "                \"embedding_input_chars\": len(part),\n",
    "            }\n",
    "            add_vector(v, meta)\n",
    "            added += 1\n",
    "\n",
    "print(\"Added vectors:\", added)\n",
    "print(\"Index ntotal:\", index.ntotal)\n",
    "\n",
    "faiss.write_index(index, INDEX_PATH)\n",
    "with open(MAP_PATH, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(mapping, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "print(\"Saved:\", INDEX_PATH, MAP_PATH)"
   ],
   "id": "1c938884959c451a",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added vectors: 410\n",
      "Index ntotal: 410\n",
      "Saved: vector_store_v3\\faiss_index.bin vector_store_v3\\mapping.json\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-06T13:32:39.706038Z",
     "start_time": "2026-01-06T13:32:39.686842Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import faiss, json\n",
    "\n",
    "index = faiss.read_index(\"vector_store_v3/faiss_index.bin\")\n",
    "with open(\"vector_store_v3/mapping.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    mapping = json.load(f)\n",
    "\n",
    "print(\"ntotal:\", index.ntotal)\n",
    "\n",
    "# afficher 3 entrées au hasard\n",
    "for k in list(mapping.keys())[:3]:\n",
    "    print(k, mapping[k][\"title\"], mapping[k][\"url\"])\n"
   ],
   "id": "52fd0a5521399169",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ntotal: 410\n",
      "0 diplome ingénieur esilv - page 1 - part 1/1 pdf://download_10873.pdf#page=1\n",
      "1 diplome ingénieur esilv - page 2 - part 1/3 pdf://download_10873.pdf#page=2\n",
      "2 diplome ingénieur esilv - page 2 - part 2/3 pdf://download_10873.pdf#page=2\n"
     ]
    }
   ],
   "execution_count": 7
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
